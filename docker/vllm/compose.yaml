services:
  model-1:
    container_name: vllm-openai-api
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    ports:
      - "3000:3000"
    ipc: host
    volumes:
      - ./chat_template:/app/chat_template
      - ~/.cache/huggingface:/root/.cache/huggingface
    devices:
      - /dev/neuron0
    environment:
      TZ: "Asia/Tokyo"
      MODEL: meta-llama/Meta-Llama-3-8B
      PORT: 3000
      CORES: 0-1
